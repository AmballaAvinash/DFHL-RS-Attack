{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "209a4dd9-14d1-450b-8762-db36db43ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from kornia import augmentation\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DModel\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from diffusers import DDPMScheduler\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from datasets import load_dataset, load_metric\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer, TrainerCallback\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a83a42-7fa1-4069-a5ab-de1a26b60fbe",
   "metadata": {},
   "source": [
    "### Define Diffusion, Victim and Stolen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfcc0562-c0a2-4a4d-9970-ec72ed8c2889",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diffuser(args):\n",
    "    scheduler_D = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\")\n",
    "    diffuser_model = UNet2DModel(\n",
    "        sample_size=(args.img_c, args.img_w, args.img_h),  # the target image resolution\n",
    "        in_channels=args.img_c,  # the number of input channels, 3 for RGB images\n",
    "        out_channels=args.img_c,  # the number of output channels\n",
    "        layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"AttnDownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        ),\n",
    "    )\n",
    "    return diffuser_model.to(args.device), scheduler_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e1a5870-28a3-44b0-936b-2288ffacc0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_victim_clone(args):\n",
    "    victim_model = ViTForImageClassification.from_pretrained(args.victim_path)\n",
    "    clone_model = ViTForImageClassification.from_pretrained(args.basemodel_path, num_labels=args.N_classes, id2label=victim_model.config.id2label, label2id=victim_model.config.label2id)\n",
    "    victim_processor = ViTImageProcessor.from_pretrained(args.victim_path)\n",
    "    clone_processor = ViTImageProcessor.from_pretrained(args.basemodel_path)\n",
    "    return victim_model.to(args.device), clone_model.to(args.device), victim_processor, clone_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7b72a-4a58-4a63-9dd1-db77d69bafec",
   "metadata": {},
   "source": [
    "### Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a72d70bf-5c0f-4601-a990-44ceaa0ae003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, device, seed, epochs, batch_size, img_n, img_c, \n",
    "                 img_w, img_h, lr_D, lr_C, lr_hee, weight_decay, momentum, N_D, N_C, \n",
    "                 steps_hee, grad_accumulation_steps, std_aug, lam, basemodel_path, \n",
    "                 victim_path, N_classes, debug, victim_transform, diffuser_transform):\n",
    "        \n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.img_n = img_n\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.lr_D = lr_D\n",
    "        self.lr_C = lr_C\n",
    "        self.lr_hee = lr_hee\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.N_D = N_D\n",
    "        self.N_C = N_C\n",
    "        self.steps_hee = steps_hee\n",
    "        self.grad_accumulation_steps = grad_accumulation_steps\n",
    "        self.std_aug = std_aug\n",
    "        self.lam = lam\n",
    "        self.basemodel_path = basemodel_path\n",
    "        self.victim_path = victim_path\n",
    "        self.N_classes = N_classes\n",
    "        self.debug = debug\n",
    "        self.victim_transform = victim_transform\n",
    "        self.diffuser_transform = diffuser_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbcf080-786f-4378-8e0f-719bc6240b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.772351Z",
     "iopub.status.busy": "2024-08-12T06:10:27.772351Z",
     "iopub.status.idle": "2024-08-12T06:10:27.788665Z",
     "shell.execute_reply": "2024-08-12T06:10:27.788665Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.772351Z"
    }
   },
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90760983-d005-439a-b334-28565629571a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.792181Z",
     "iopub.status.busy": "2024-08-12T06:10:27.792181Z",
     "iopub.status.idle": "2024-08-12T06:10:27.804109Z",
     "shell.execute_reply": "2024-08-12T06:10:27.803025Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.792181Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataIter(object):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self._iter = iter(self.dataloader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            data = next(self._iter)\n",
    "        except StopIteration:\n",
    "            self._iter = iter(self.dataloader)\n",
    "            data = next(self._iter)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283f433e-1666-439c-9120-40ab921109ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.805108Z",
     "iopub.status.busy": "2024-08-12T06:10:27.805108Z",
     "iopub.status.idle": "2024-08-12T06:10:27.812210Z",
     "shell.execute_reply": "2024-08-12T06:10:27.812210Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.805108Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_standard_augment(img_w, img_h):\n",
    "    std_aug = augmentation.container.ImageSequential(\n",
    "    augmentation.RandomCrop(size=[img_w, img_h], padding=4),\n",
    "    augmentation.RandomHorizontalFlip(),\n",
    ")\n",
    "    return std_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc23b46-cae7-4f8a-a471-986e7a97d28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.812210Z",
     "iopub.status.busy": "2024-08-12T06:10:27.812210Z",
     "iopub.status.idle": "2024-08-12T06:10:27.824359Z",
     "shell.execute_reply": "2024-08-12T06:10:27.824359Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.812210Z"
    }
   },
   "outputs": [],
   "source": [
    "def strong_aug(image):\n",
    "    device = image.device\n",
    "    image = TF.center_crop(\n",
    "        image,\n",
    "        [int(32.0 * random.uniform(0.95, 1.0)), int(32.0 * random.uniform(0.95, 1.0))],\n",
    "    )\n",
    "    image = TF.resize(image, [32, 32])\n",
    "    noise = torch.randn_like(image).to(device) * 0.001\n",
    "    image = torch.clamp(image + noise, 0.0, 1.0)\n",
    "    if random.uniform(0, 1) > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "    if random.uniform(0, 1) > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "    angles = [-15, 0, 15]\n",
    "    angle = random.choice(angles)\n",
    "    image = TF.rotate(image, angle)\n",
    "    image = TF.resize(image, [224,224])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d75aaf-ae69-479d-9483-32cacc82991c",
   "metadata": {},
   "source": [
    "### DFME++ Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2614f5c-9a4b-4441-b4f8-028b44634bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.824359Z",
     "iopub.status.busy": "2024-08-12T06:10:27.824359Z",
     "iopub.status.idle": "2024-08-12T06:10:27.840234Z",
     "shell.execute_reply": "2024-08-12T06:10:27.840234Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.824359Z"
    }
   },
   "outputs": [],
   "source": [
    "class Entropy_Loss(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(Entropy_Loss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        b = -1.0 * b.sum(dim=1)\n",
    "        if self.reduction == \"mean\":\n",
    "            return b.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return b.sum()\n",
    "        elif self.reduction == \"none\":\n",
    "            return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a686e71-ba43-4e9c-b036-8575c8b02554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.842252Z",
     "iopub.status.busy": "2024-08-12T06:10:27.841234Z",
     "iopub.status.idle": "2024-08-12T06:10:27.848204Z",
     "shell.execute_reply": "2024-08-12T06:10:27.848204Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.842252Z"
    }
   },
   "outputs": [],
   "source": [
    "def div_loss(outpus):\n",
    "    softmax_o_S = F.softmax(outpus, dim=1).mean(dim=0)\n",
    "    loss_div = (softmax_o_S * torch.log10(softmax_o_S)).sum()\n",
    "    return loss_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b20e757-516e-4532-8120-378f3fbb0a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.848204Z",
     "iopub.status.busy": "2024-08-12T06:10:27.848204Z",
     "iopub.status.idle": "2024-08-12T06:10:27.863830Z",
     "shell.execute_reply": "2024-08-12T06:10:27.863430Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.848204Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_hee(args, model, x):\n",
    "    model.eval()\n",
    "    x_hee = x.detach() + 0.001 * torch.torch.randn(x.shape).to(args.device).detach()\n",
    "    for _ in range(args.steps_hee):\n",
    "        x_hee.requires_grad_()\n",
    "        with torch.enable_grad():\n",
    "            pred = model(x_hee).logits\n",
    "            loss = Entropy_Loss(reduction=\"mean\")(pred)\n",
    "        grad = torch.autograd.grad(loss, [x_hee])[0]\n",
    "        x_hee = x_hee.detach() + args.lr_hee * torch.sign(grad.detach())\n",
    "        x_hee = torch.clamp(x_hee, 0.0, 1.0)\n",
    "    model.train()\n",
    "\n",
    "    return x_hee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d41017-83cc-47ad-8866-34e2dd6ff5c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.863830Z",
     "iopub.status.busy": "2024-08-12T06:10:27.863830Z",
     "iopub.status.idle": "2024-08-12T06:10:27.879824Z",
     "shell.execute_reply": "2024-08-12T06:10:27.879158Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.863830Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer_diffuser(args, diffuser_model, scheduler_D):\n",
    "    noise = torch.randn((args.img_n, args.img_c, args.img_w, args.img_h)).to(args.device)\n",
    "    for i, t in (enumerate(scheduler_D.timesteps)):\n",
    "        #if args.debug: print(f'Debug(Diffusion_Inference) :-> Steps Taken:{t}')\n",
    "        with torch.no_grad():\n",
    "            residual = diffuser_model(noise, t.to(args.device),return_dict=False)[0]\n",
    "        noise = scheduler_D.step(residual, t, noise).prev_sample\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d20fd1-1e39-48ba-8002-e1960c23f439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.879824Z",
     "iopub.status.busy": "2024-08-12T06:10:27.879824Z",
     "iopub.status.idle": "2024-08-12T06:10:27.908271Z",
     "shell.execute_reply": "2024-08-12T06:10:27.907666Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.879824Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_diffuser(args, diffuser_model, clone_model, scheduler_D, clone_processor):\n",
    "\n",
    "    if args.debug: print('Debug(train_diffuser) :-> Generating Images using Diffusion Model')\n",
    "        \n",
    "    img_diff = infer_diffuser(args, diffuser_model, scheduler_D)\n",
    "    img_diff = args.victim_transform(img_diff)\n",
    "    img_diff = clone_processor(img_diff , return_tensors='pt').to(device)['pixel_values']\n",
    "    \n",
    "    if args.debug: \n",
    "        print('Debug(train_diffuser) :-> Images Generated Using Diffusion Model')\n",
    "        print('Debug(train_diffuser) :-> Generating HEE samples')\n",
    "        \n",
    "    img_hee = generate_hee(args, clone_model, img_diff)\n",
    "    img_hee = args.diffuser_transform(img_hee)\n",
    "    if args.debug: print('Debug(train_diffuser) :-> HEE samples generated')\n",
    "    img_hee = args.std_aug(img_hee)\n",
    "    tensor_dataset = TensorDataset(img_hee)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    diffuser_model.train()\n",
    "    clone_model.eval()\n",
    "\n",
    "    optimizer_D = torch.optim.Adam([{\"params\" : diffuser_model.parameters()}], lr=args.lr_D, betas=[0.5, 0.999])\n",
    "    losses = []\n",
    "    \n",
    "    if args.debug: print('Debug(train_diffuser) :-> Starting Diffusion Training')\n",
    "    for epoch in range(args.N_D):\n",
    "        for step, batch in (enumerate(data_loader)):\n",
    "            noise = torch.randn((batch.shape[0], args.img_c, args.img_w, args.img_h)).to(args.device)\n",
    "            timesteps = torch.randint(low = 0,high = 999,size=(batch.shape[0],)).long().to(args.device)\n",
    "            noisy_x = scheduler_D.add_noise(batch, noise, timesteps).to(args.device) \n",
    "\n",
    "            noisy_pred = diffuser_model(noisy_x, timesteps,return_dict=False)[0]\n",
    "            loss = F.mse_loss(noisy_pred, noise)\n",
    "            with torch.no_grad():\n",
    "                img_gen_clone = clone_processor(args.victim_transform(noisy_pred), return_tensors='pt').to(device)['pixel_values']\n",
    "                clone_pred = clone_model(img_gen_clone).logits\n",
    "            loss_div = div_loss(clone_pred)\n",
    "            loss = loss - loss_div * args.lam\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            if (step +1 ) % args.grad_accumulation_steps == 0:\n",
    "                optimizer_D.step()\n",
    "                optimizer_D.zero_grad()\n",
    "        \n",
    "        print(f\"Diffusion Epoch {epoch} average loss: {sum(losses[-len(data_loader):])/len(data_loader)}\")\n",
    "    if args.debug: print('Debug(train_diffuser) :-> Diffusion Training Ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ca31e0-8315-459e-bc2b-f013c5091674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.908271Z",
     "iopub.status.busy": "2024-08-12T06:10:27.908271Z",
     "iopub.status.idle": "2024-08-12T06:10:27.924659Z",
     "shell.execute_reply": "2024-08-12T06:10:27.923307Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.908271Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_clone(args, diffuser_model, clone_model, victim_model, scheduler_D, optimizer_C, victim_processor, clone_processor):\n",
    "\n",
    "    diffuser_model.eval()\n",
    "    clone_model.train()\n",
    "    victim_model.eval()\n",
    "\n",
    "    if args.debug: print('Debug(train_clone) :-> Generating Images using Diffusion Model')\n",
    "    img_adv = infer_diffuser(args, diffuser_model, scheduler_D)\n",
    "    img_adv = args.victim_transform(img_adv)\n",
    "    if args.debug: print('Debug(train_clone) :-> Images Generated Using Diffusion Model')\n",
    "    img_adv = strong_aug(args.std_aug(img_adv))\n",
    "    tensor_dataset = TensorDataset(img_adv)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    data_iter = DataIter(data_loader)\n",
    "\n",
    "    classes_fired = np.zeros(args.N_classes)\n",
    "    losses = []\n",
    "    if args.debug: print('Debug(train_clone) :-> Starting Clone Model Training')        \n",
    "    for step in range(args.N_C):\n",
    "        img_gen = data_iter.next()\n",
    "        img_gen_victim = victim_processor(img_gen, return_tensors='pt').to(device)['pixel_values']\n",
    "        img_gen_clone = clone_processor(img_gen, return_tensors='pt').to(device)['pixel_values']\n",
    "        logits_T = victim_model(img_gen).logits.detach() #hard_labels = logits_T.topk(1, 1)[1].reshape(-1)\n",
    "        hard_labels = logits_T.topk(1, 1)[1].reshape(-1)\n",
    "        np.add.at(classes_fired, hard_labels , 1)\n",
    "        logits_C = clone_model(img_gen).logits\n",
    "    \n",
    "        loss = F.cross_entropy(logits_C, hard_labels)\n",
    "        print(loss)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        if (step +1 ) % args.grad_accumulation_steps == 0:\n",
    "            optimizer_C.step()\n",
    "            optimizer_C.zero_grad()\n",
    "            print(f\"Clone Steps {step} average loss: {sum(losses[-len(data_loader):])/len(data_loader)}\")\n",
    "            print(f\"Clone Steps {step} Classes Fired: {classes_fired})\")\n",
    "    if args.debug: print('Debug(train_clone) :-> Clone Model Training Ended')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb4a0d0-24b1-4475-b383-2091324cdc64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.925721Z",
     "iopub.status.busy": "2024-08-12T06:10:27.925721Z",
     "iopub.status.idle": "2024-08-12T06:10:27.946991Z",
     "shell.execute_reply": "2024-08-12T06:10:27.945992Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.925721Z"
    }
   },
   "outputs": [],
   "source": [
    "debug = 1 #To debug code\n",
    "device = torch.device('cuda') #device placement cpu or gpu\n",
    "seed = 10 #seed for consistent result\n",
    "epochs = 300 #number of epochs to train\n",
    "batch_size = 16 #per device batch size\n",
    "img_n = 80 #min(160, batch_size*10*2) #per epoch image generation count\n",
    "img_c = 3 #image channel\n",
    "img_w = 32 #image size\n",
    "img_h = 32 #image size\n",
    "lr_D = 0.002 #learning rate of Diffuser\n",
    "lr_C = 0.1 #learing rate of clone model\n",
    "lr_hee = 0.03 #perturb number of steps\n",
    "weight_decay = 1e-4 #Optimizer parameter: decay's weight update\n",
    "momentum = 0.9 #Optimizer parameter: Remeber past information 1/momentum times\n",
    "N_D = 100 #Diffuser train epochs\n",
    "N_C = 500 #Clone model steps \n",
    "steps_hee = 10 #number of epochs to train\n",
    "grad_accumulation_steps = 16 #update model after no.of steps\n",
    "std_aug = get_standard_augment(img_w, img_h) #standard augmentation: flip, crop\n",
    "lam = 3 #hyperparameter for balancing two loss terms in diffuser\n",
    "basemodel_path = \"C:\\GVR3KOR_WORK\\Models\\Huggingface\\ViT\\Base_16_patch\" #clone model path\n",
    "victim_path = \"C:\\GVR3KOR_WORK\\CV\\DFME\\HEE\\Vit_Base_Beans\" #victim model path\n",
    "N_classes = 3 #No.of classes to predict\n",
    "victim_transform  = transforms.Resize((224, 224)) #to transform to victim shape\n",
    "diffuser_transform  = transforms.Resize((32, 32)) #to transform back to diffusion shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf41408d-6de7-4bbe-b9fb-8c3e2e641a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.947992Z",
     "iopub.status.busy": "2024-08-12T06:10:27.947992Z",
     "iopub.status.idle": "2024-08-12T06:10:27.957240Z",
     "shell.execute_reply": "2024-08-12T06:10:27.957240Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.947992Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Args(\n",
    "        debug = debug,\n",
    "        device = device,\n",
    "        seed = seed,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        img_n = img_n,\n",
    "        img_c = img_c,\n",
    "        img_w = img_w,\n",
    "        img_h = img_h,\n",
    "        lr_D = lr_D,\n",
    "        lr_C = lr_C,\n",
    "        lr_hee = lr_hee,\n",
    "        weight_decay = weight_decay,\n",
    "        momentum = momentum,\n",
    "        N_D = N_D,\n",
    "        N_C = N_C,\n",
    "        steps_hee = steps_hee,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        std_aug = std_aug,\n",
    "        lam = lam,\n",
    "        basemodel_path = basemodel_path,\n",
    "        victim_path = victim_path,\n",
    "        N_classes = N_classes,\n",
    "        victim_transform  = victim_transform,\n",
    "        diffuser_transform = diffuser_transform\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d587785-14be-4afe-a16a-f6b15b88d3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:27.957240Z",
     "iopub.status.busy": "2024-08-12T06:10:27.957240Z",
     "iopub.status.idle": "2024-08-12T06:10:28.006101Z",
     "shell.execute_reply": "2024-08-12T06:10:28.004794Z",
     "shell.execute_reply.started": "2024-08-12T06:10:27.957240Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84d79aed-b9c5-4f0a-ab98-646d8c2a7fe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:28.006101Z",
     "iopub.status.busy": "2024-08-12T06:10:28.006101Z",
     "iopub.status.idle": "2024-08-12T06:10:28.923582Z",
     "shell.execute_reply": "2024-08-12T06:10:28.922707Z",
     "shell.execute_reply.started": "2024-08-12T06:10:28.006101Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at C:\\GVR3KOR_WORK\\Models\\Huggingface\\ViT\\Base_16_patch and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "diffuser_model, scheduler_D = get_diffuser(args)\n",
    "victim_model, clone_model, victim_processor, clone_processor = get_victim_clone(args)\n",
    "victim_processor.do_resize, clone_processor.do_resize = False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18130477-043a-4282-ace6-554523132498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:28.923582Z",
     "iopub.status.busy": "2024-08-12T06:10:28.923582Z",
     "iopub.status.idle": "2024-08-12T06:10:28.940126Z",
     "shell.execute_reply": "2024-08-12T06:10:28.938702Z",
     "shell.execute_reply.started": "2024-08-12T06:10:28.923582Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_C = torch.optim.SGD(\n",
    "        clone_model.parameters(),\n",
    "        lr=args.lr_C,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "scheduler_lr = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_C, args.epochs, eta_min=2e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7e96c-9df4-4f02-a4db-21fcfca02c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-12T06:10:28.940126Z",
     "iopub.status.busy": "2024-08-12T06:10:28.940126Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug(train_diffuser) :-> Generating Images using Diffusion Model\n",
      "Debug(train_diffuser) :-> Images Generated Using Diffusion Model\n",
      "Debug(train_diffuser) :-> Generating HEE samples\n"
     ]
    }
   ],
   "source": [
    " for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "     \n",
    "        train_diffuser(args, diffuser_model, clone_model, scheduler_D, clone_processor)\n",
    "        train_clone(args, diffuser_model, clone_model, victim_model, scheduler_D, optimizer_C, victim_processor, clone_processor)\n",
    "        scheduler_lr.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609f975d-fe0d-4a5a-b24c-69f80960da1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32406030-c079-46ea-a595-43245c4dfa68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

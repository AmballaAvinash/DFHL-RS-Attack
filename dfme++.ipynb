{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "209a4dd9-14d1-450b-8762-db36db43ef07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:50:47.801770Z",
     "iopub.status.busy": "2024-08-07T06:50:47.800769Z",
     "iopub.status.idle": "2024-08-07T06:51:13.347639Z",
     "shell.execute_reply": "2024-08-07T06:51:13.346638Z",
     "shell.execute_reply.started": "2024-08-07T06:50:47.800769Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from kornia import augmentation\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DModel\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from diffusers import DDPMScheduler\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from datasets import load_dataset, load_metric\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, TrainingArguments, Trainer, TrainerCallback\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a83a42-7fa1-4069-a5ab-de1a26b60fbe",
   "metadata": {},
   "source": [
    "### Define Diffusion, Victim and Stolen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfcc0562-c0a2-4a4d-9970-ec72ed8c2889",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.349638Z",
     "iopub.status.busy": "2024-08-07T06:51:13.348637Z",
     "iopub.status.idle": "2024-08-07T06:51:13.361849Z",
     "shell.execute_reply": "2024-08-07T06:51:13.361849Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.349638Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_diffuser(args):\n",
    "    scheduler_D = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\")\n",
    "    diffuser_model = UNet2DModel(\n",
    "        sample_size=(args.img_c, args.img_w, args.img_h),  # the target image resolution\n",
    "        in_channels=args.img_c,  # the number of input channels, 3 for RGB images\n",
    "        out_channels=args.img_c,  # the number of output channels\n",
    "        layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
    "        block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
    "        down_block_types=(\n",
    "            \"DownBlock2D\",  # a regular ResNet downsampling block\n",
    "            \"DownBlock2D\",\n",
    "            \"DownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "            \"AttnDownBlock2D\",\n",
    "        ),\n",
    "        up_block_types=(\n",
    "            \"AttnUpBlock2D\",\n",
    "            \"UpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "            \"UpBlock2D\",\n",
    "            \"UpBlock2D\",  # a regular ResNet upsampling block\n",
    "        ),\n",
    "    )\n",
    "    return diffuser_model.to(args.device), scheduler_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e1a5870-28a3-44b0-936b-2288ffacc0c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.363851Z",
     "iopub.status.busy": "2024-08-07T06:51:13.363851Z",
     "iopub.status.idle": "2024-08-07T06:51:13.377895Z",
     "shell.execute_reply": "2024-08-07T06:51:13.377895Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.363851Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_victim_clone(args):\n",
    "    victim_model = ViTForImageClassification.from_pretrained(args.victim_path)\n",
    "    clone_model = ViTForImageClassification.from_pretrained(args.basemodel_path, num_labels=args.N_classes, id2label=victim_model.config.id2label, label2id=victim_model.config.label2id)\n",
    "    victim_processor = ViTImageProcessor.from_pretrained(args.victim_path)\n",
    "    clone_processor = ViTImageProcessor.from_pretrained(args.basemodel_path)\n",
    "    return victim_model.to(args.device), clone_model.to(args.device), victim_processor, clone_processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7b72a-4a58-4a63-9dd1-db77d69bafec",
   "metadata": {},
   "source": [
    "### Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a72d70bf-5c0f-4601-a990-44ceaa0ae003",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.379897Z",
     "iopub.status.busy": "2024-08-07T06:51:13.379897Z",
     "iopub.status.idle": "2024-08-07T06:51:13.393416Z",
     "shell.execute_reply": "2024-08-07T06:51:13.393416Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.379897Z"
    }
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, device, seed, epochs, batch_size, img_n, img_c, \n",
    "                 img_w, img_h, lr_D, lr_C, lr_hee, weight_decay, momentum, N_D, N_C, \n",
    "                 steps_hee, grad_accumulation_steps, std_aug, lam, basemodel_path, \n",
    "                 victim_path, N_classes, debug, victim_transform, diffuser_transform):\n",
    "        \n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.img_n = img_n\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.lr_D = lr_D\n",
    "        self.lr_C = lr_C\n",
    "        self.lr_hee = lr_hee\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.N_D = N_D\n",
    "        self.N_C = N_C\n",
    "        self.steps_hee = steps_hee\n",
    "        self.grad_accumulation_steps = grad_accumulation_steps\n",
    "        self.std_aug = std_aug\n",
    "        self.lam = lam\n",
    "        self.basemodel_path = basemodel_path\n",
    "        self.victim_path = victim_path\n",
    "        self.N_classes = N_classes\n",
    "        self.debug = debug\n",
    "        self.victim_transform = victim_transform\n",
    "        self.diffuser_transform = diffuser_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dbcf080-786f-4378-8e0f-719bc6240b7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.395932Z",
     "iopub.status.busy": "2024-08-07T06:51:13.394923Z",
     "iopub.status.idle": "2024-08-07T06:51:13.408960Z",
     "shell.execute_reply": "2024-08-07T06:51:13.408960Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.395932Z"
    }
   },
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90760983-d005-439a-b334-28565629571a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.412967Z",
     "iopub.status.busy": "2024-08-07T06:51:13.412967Z",
     "iopub.status.idle": "2024-08-07T06:51:13.424832Z",
     "shell.execute_reply": "2024-08-07T06:51:13.424832Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.412967Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataIter(object):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self._iter = iter(self.dataloader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            data = next(self._iter)\n",
    "        except StopIteration:\n",
    "            self._iter = iter(self.dataloader)\n",
    "            data = next(self._iter)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "283f433e-1666-439c-9120-40ab921109ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.426785Z",
     "iopub.status.busy": "2024-08-07T06:51:13.425767Z",
     "iopub.status.idle": "2024-08-07T06:51:13.441322Z",
     "shell.execute_reply": "2024-08-07T06:51:13.440322Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.426785Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_standard_augment(img_w, img_h):\n",
    "    std_aug = augmentation.container.ImageSequential(\n",
    "    augmentation.RandomCrop(size=[img_w, img_h], padding=4),\n",
    "    augmentation.RandomHorizontalFlip(),\n",
    ")\n",
    "    return std_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc23b46-cae7-4f8a-a471-986e7a97d28e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.442322Z",
     "iopub.status.busy": "2024-08-07T06:51:13.442322Z",
     "iopub.status.idle": "2024-08-07T06:51:13.456915Z",
     "shell.execute_reply": "2024-08-07T06:51:13.455913Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.442322Z"
    }
   },
   "outputs": [],
   "source": [
    "def strong_aug(image):\n",
    "    device = image.device\n",
    "    image = TF.center_crop(\n",
    "        image,\n",
    "        [int(32.0 * random.uniform(0.95, 1.0)), int(32.0 * random.uniform(0.95, 1.0))],\n",
    "    )\n",
    "    image = TF.resize(image, [32, 32])\n",
    "    noise = torch.randn_like(image).to(device) * 0.001\n",
    "    image = torch.clamp(image + noise, 0.0, 1.0)\n",
    "    if random.uniform(0, 1) > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "    if random.uniform(0, 1) > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "    angles = [-15, 0, 15]\n",
    "    angle = random.choice(angles)\n",
    "    image = TF.rotate(image, angle)\n",
    "    image = TF.resize(image, [224,224])\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d75aaf-ae69-479d-9483-32cacc82991c",
   "metadata": {},
   "source": [
    "### DFME++ Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2614f5c-9a4b-4441-b4f8-028b44634bed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.457915Z",
     "iopub.status.busy": "2024-08-07T06:51:13.457915Z",
     "iopub.status.idle": "2024-08-07T06:51:13.472005Z",
     "shell.execute_reply": "2024-08-07T06:51:13.471008Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.457915Z"
    }
   },
   "outputs": [],
   "source": [
    "class Entropy_Loss(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(Entropy_Loss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x):\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        b = -1.0 * b.sum(dim=1)\n",
    "        if self.reduction == \"mean\":\n",
    "            return b.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return b.sum()\n",
    "        elif self.reduction == \"none\":\n",
    "            return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a686e71-ba43-4e9c-b036-8575c8b02554",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.473007Z",
     "iopub.status.busy": "2024-08-07T06:51:13.473007Z",
     "iopub.status.idle": "2024-08-07T06:51:13.487656Z",
     "shell.execute_reply": "2024-08-07T06:51:13.486655Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.473007Z"
    }
   },
   "outputs": [],
   "source": [
    "def div_loss(outpus):\n",
    "    softmax_o_S = F.softmax(outpus, dim=1).mean(dim=0)\n",
    "    loss_div = (softmax_o_S * torch.log10(softmax_o_S)).sum()\n",
    "    return loss_div"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b20e757-516e-4532-8120-378f3fbb0a99",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.488660Z",
     "iopub.status.busy": "2024-08-07T06:51:13.488660Z",
     "iopub.status.idle": "2024-08-07T06:51:13.503295Z",
     "shell.execute_reply": "2024-08-07T06:51:13.502294Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.488660Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_hee(args, model, x):\n",
    "    model.eval()\n",
    "    x_hee = x.detach() + 0.001 * torch.torch.randn(x.shape).to(args.device).detach()\n",
    "    for _ in range(args.steps_hee):\n",
    "        x_hee.requires_grad_()\n",
    "        with torch.enable_grad():\n",
    "            pred = model(x_hee).logits\n",
    "            loss = Entropy_Loss(reduction=\"mean\")(pred)\n",
    "        grad = torch.autograd.grad(loss, [x_hee])[0]\n",
    "        x_hee = x_hee.detach() + args.lr_hee * torch.sign(grad.detach())\n",
    "        x_hee = torch.clamp(x_hee, 0.0, 1.0)\n",
    "    model.train()\n",
    "\n",
    "    return x_hee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15d41017-83cc-47ad-8866-34e2dd6ff5c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.504803Z",
     "iopub.status.busy": "2024-08-07T06:51:13.503295Z",
     "iopub.status.idle": "2024-08-07T06:51:13.518340Z",
     "shell.execute_reply": "2024-08-07T06:51:13.518340Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.504803Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer_diffuser(args, diffuser_model, scheduler_D):\n",
    "    noise = torch.randn((args.img_n, args.img_c, args.img_w, args.img_h)).to(args.device)\n",
    "    for i, t in (enumerate(scheduler_D.timesteps)):\n",
    "        #if args.debug: print(f'Debug(Diffusion_Inference) :-> Steps Taken:{t}')\n",
    "        with torch.no_grad():\n",
    "            residual = diffuser_model(noise, t.to(args.device),return_dict=False)[0]\n",
    "        noise = scheduler_D.step(residual, t, noise).prev_sample\n",
    "    return noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23d20fd1-1e39-48ba-8002-e1960c23f439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.520340Z",
     "iopub.status.busy": "2024-08-07T06:51:13.520340Z",
     "iopub.status.idle": "2024-08-07T06:51:13.534864Z",
     "shell.execute_reply": "2024-08-07T06:51:13.533863Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.520340Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_diffuser(args, diffuser_model, clone_model, scheduler_D, clone_processor):\n",
    "\n",
    "    if args.debug: print('Debug(train_diffuser) :-> Generating Images using Diffusion Model')\n",
    "        \n",
    "    img_diff = infer_diffuser(args, diffuser_model, scheduler_D)\n",
    "    img_diff = args.victim_transform(img_diff)\n",
    "    img_diff = clone_processor(img_diff , return_tensors='pt').to(device)['pixel_values']\n",
    "    # img_diff = torch.tensor(numpy.array(img_diff['pixel_values'])).to(device)\n",
    "    \n",
    "    if args.debug: \n",
    "        print('Debug(train_diffuser) :-> Images Generated Using Diffusion Model')\n",
    "        print('Debug(train_diffuser) :-> Generating HEE samples')\n",
    "        \n",
    "    img_hee = generate_hee(args, clone_model, img_diff)\n",
    "    img_hee = args.diffuser_transform(img_hee)\n",
    "    if args.debug: print('Debug(train_diffuser) :-> HEE samples generated')\n",
    "    img_hee = args.std_aug(img_hee)\n",
    "    tensor_dataset = TensorDataset(img_hee)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    \n",
    "    diffuser_model.train()\n",
    "    clone_model.eval()\n",
    "\n",
    "    optimizer_D = torch.optim.Adam([{\"params\" : diffuser_model.parameters()}], lr=args.lr_D, betas=[0.5, 0.999])\n",
    "    losses = []\n",
    "    \n",
    "    if args.debug: print('Debug(train_diffuser) :-> Starting Diffusion Training')\n",
    "    for epoch in range(args.N_D):\n",
    "        for step, batch in (enumerate(data_loader)):\n",
    "            noise = torch.randn((batch.shape[0], args.img_c, args.img_w, args.img_h)).to(args.device)\n",
    "            timesteps = torch.randint(low = 0,high = 999,size=(batch.shape[0],)).long().to(args.device)\n",
    "            noisy_x = scheduler_D.add_noise(batch, noise, timesteps).to(args.device) \n",
    "\n",
    "            noisy_pred = diffuser_model(noisy_x, timesteps,return_dict=False)[0]\n",
    "            loss = F.mse_loss(noisy_pred, noise)\n",
    "            with torch.no_grad():\n",
    "                img_gen_clone = clone_processor(args.victim_transform(noisy_pred), return_tensors='pt').to(device)['pixel_values']\n",
    "                clone_pred = clone_model(img_gen_clone).logits\n",
    "            loss_div = div_loss(clone_pred)\n",
    "            loss = loss - loss_div * args.lam\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            if (step +1 ) % args.grad_accumulation_steps == 0:\n",
    "                optimizer_D.step()\n",
    "                optimizer_D.zero_grad()\n",
    "        \n",
    "        print(f\"Diffusion Epoch {epoch} average loss: {sum(losses[-len(data_loader):])/len(data_loader)}\")\n",
    "    if args.debug: print('Debug(train_diffuser) :-> Diffusion Training Ended')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39ca31e0-8315-459e-bc2b-f013c5091674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.537207Z",
     "iopub.status.busy": "2024-08-07T06:51:13.536219Z",
     "iopub.status.idle": "2024-08-07T06:51:13.550725Z",
     "shell.execute_reply": "2024-08-07T06:51:13.549725Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.537207Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_clone(args, diffuser_model, clone_model, victim_model, scheduler_D, optimizer_C, victim_processor, clone_processor):\n",
    "\n",
    "    diffuser_model.eval()\n",
    "    clone_model.train()\n",
    "    victim_model.eval()\n",
    "\n",
    "    if args.debug: print('Debug(train_clone) :-> Generating Images using Diffusion Model')\n",
    "    img_adv = infer_diffuser(args, diffuser_model, scheduler_D)\n",
    "    img_adv = args.victim_transform(img_adv)\n",
    "    if args.debug: print('Debug(train_clone) :-> Images Generated Using Diffusion Model')\n",
    "    img_adv = strong_aug(args.std_aug(img_adv))\n",
    "    tensor_dataset = TensorDataset(img_adv)\n",
    "    data_loader = DataLoader(tensor_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "    data_iter = DataIter(data_loader)\n",
    "    \n",
    "    losses = []\n",
    "    if args.debug: print('Debug(train_clone) :-> Starting Clone Model Training')        \n",
    "    for step in range(args.N_C):\n",
    "        img_gen = data_iter.next()\n",
    "        print(img_gen.shape)\n",
    "        img_gen_victim = victim_processor(img_gen, return_tensors='pt').to(device)['pixel_values']\n",
    "        img_gen_clone = clone_processor(img_gen, return_tensors='pt').to(device)['pixel_values']\n",
    "        logits_T = victim_model(img_gen_victim).logits.detach() #hard_labels = logits_T.topk(1, 1)[1].reshape(-1)\n",
    "        logits_C = clone_model(img_gen_clone).logits\n",
    "\n",
    "        loss = F.cross_entropy(logits_T, logits_C)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        if (step +1 ) % args.grad_accumulation_steps == 0:\n",
    "            optimizer_C.step()\n",
    "            optimizer_C.zero_grad()\n",
    "            print(f\"Clone Steps {step} average loss: {sum(losses[-len(data_loader):])/len(data_loader)}\")\n",
    "    if args.debug: print('Debug(train_clone) :-> Clone Model Training Ended')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb4a0d0-24b1-4475-b383-2091324cdc64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.551725Z",
     "iopub.status.busy": "2024-08-07T06:51:13.551725Z",
     "iopub.status.idle": "2024-08-07T06:51:13.565428Z",
     "shell.execute_reply": "2024-08-07T06:51:13.564921Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.551725Z"
    }
   },
   "outputs": [],
   "source": [
    "debug = 1 #To debug code\n",
    "device = torch.device('cuda') #device placement cpu or gpu\n",
    "seed = 10 #seed for consistent result\n",
    "epochs = 300 #number of epochs to train\n",
    "batch_size = 16 #per device batch size\n",
    "img_n = 80 #min(160, batch_size*10*2) #per epoch image generation count\n",
    "img_c = 3 #image channel\n",
    "img_w = 32 #image size\n",
    "img_h = 32 #image size\n",
    "lr_D = 0.002 #learning rate of Diffuser\n",
    "lr_C = 0.1 #learing rate of clone model\n",
    "lr_hee = 0.03 #perturb number of steps\n",
    "weight_decay = 1e-4 #Optimizer parameter: decay's weight update\n",
    "momentum = 0.9 #Optimizer parameter: Remeber past information 1/momentum times\n",
    "N_D = 100 #Diffuser train epochs\n",
    "N_C = 500 #Clone model steps \n",
    "steps_hee = 10 #number of epochs to train\n",
    "grad_accumulation_steps = 16 #update model after no.of steps\n",
    "std_aug = get_standard_augment(img_w, img_h) #standard augmentation: flip, crop\n",
    "lam = 3 #hyperparameter for balancing two loss terms in diffuser\n",
    "basemodel_path = \"C:\\GVR3KOR_WORK\\Models\\Huggingface\\ViT\\Base_16_patch\" #clone model path\n",
    "victim_path = \"C:\\GVR3KOR_WORK\\CV\\DFME\\HEE\\Vit_Base_Beans\" #victim model path\n",
    "N_classes = 3 #No.of classes to predict\n",
    "victim_transform  = transforms.Resize((224, 224)) #to transform to victim shape\n",
    "diffuser_transform  = transforms.Resize((32, 32)) #to transform back to diffusion shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf41408d-6de7-4bbe-b9fb-8c3e2e641a0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.566435Z",
     "iopub.status.busy": "2024-08-07T06:51:13.566435Z",
     "iopub.status.idle": "2024-08-07T06:51:13.580962Z",
     "shell.execute_reply": "2024-08-07T06:51:13.580962Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.566435Z"
    }
   },
   "outputs": [],
   "source": [
    "args = Args(\n",
    "        debug = debug,\n",
    "        device = device,\n",
    "        seed = seed,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        img_n = img_n,\n",
    "        img_c = img_c,\n",
    "        img_w = img_w,\n",
    "        img_h = img_h,\n",
    "        lr_D = lr_D,\n",
    "        lr_C = lr_C,\n",
    "        lr_hee = lr_hee,\n",
    "        weight_decay = weight_decay,\n",
    "        momentum = momentum,\n",
    "        N_D = N_D,\n",
    "        N_C = N_C,\n",
    "        steps_hee = steps_hee,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        std_aug = std_aug,\n",
    "        lam = lam,\n",
    "        basemodel_path = basemodel_path,\n",
    "        victim_path = victim_path,\n",
    "        N_classes = N_classes,\n",
    "        victim_transform  = victim_transform,\n",
    "        diffuser_transform = diffuser_transform\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d587785-14be-4afe-a16a-f6b15b88d3f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.582963Z",
     "iopub.status.busy": "2024-08-07T06:51:13.581962Z",
     "iopub.status.idle": "2024-08-07T06:51:13.645928Z",
     "shell.execute_reply": "2024-08-07T06:51:13.645024Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.582963Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84d79aed-b9c5-4f0a-ab98-646d8c2a7fe6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:13.646933Z",
     "iopub.status.busy": "2024-08-07T06:51:13.646933Z",
     "iopub.status.idle": "2024-08-07T06:51:14.836790Z",
     "shell.execute_reply": "2024-08-07T06:51:14.836790Z",
     "shell.execute_reply.started": "2024-08-07T06:51:13.646933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at C:\\GVR3KOR_WORK\\Models\\Huggingface\\ViT\\Base_16_patch and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "diffuser_model, scheduler_D = get_diffuser(args)\n",
    "victim_model, clone_model, victim_processor, clone_processor = get_victim_clone(args)\n",
    "victim_processor.do_resize, clone_processor.do_resize = False, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18130477-043a-4282-ace6-554523132498",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:14.838792Z",
     "iopub.status.busy": "2024-08-07T06:51:14.838792Z",
     "iopub.status.idle": "2024-08-07T06:51:14.852143Z",
     "shell.execute_reply": "2024-08-07T06:51:14.852143Z",
     "shell.execute_reply.started": "2024-08-07T06:51:14.838792Z"
    }
   },
   "outputs": [],
   "source": [
    "optimizer_C = torch.optim.SGD(\n",
    "        clone_model.parameters(),\n",
    "        lr=args.lr_C,\n",
    "        momentum=args.momentum,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "scheduler_lr = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_C, args.epochs, eta_min=2e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cd7e96c-9df4-4f02-a4db-21fcfca02c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-07T06:51:14.854137Z",
     "iopub.status.busy": "2024-08-07T06:51:14.854137Z",
     "iopub.status.idle": "2024-08-07T07:00:17.417933Z",
     "shell.execute_reply": "2024-08-07T07:00:17.417933Z",
     "shell.execute_reply.started": "2024-08-07T06:51:14.854137Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/300 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug(train_diffuser) :-> Generating Images using Diffusion Model\n",
      "Debug(train_diffuser) :-> Images Generated Using Diffusion Model\n",
      "Debug(train_diffuser) :-> Generating HEE samples\n",
      "Debug(train_diffuser) :-> HEE samples generated\n",
      "Debug(train_diffuser) :-> Starting Diffusion Training\n",
      "Diffusion Epoch 0 average loss: 2.612939691543579\n",
      "Diffusion Epoch 1 average loss: 2.6123223304748535\n",
      "Diffusion Epoch 2 average loss: 2.6145135402679442\n",
      "Diffusion Epoch 3 average loss: 2.616474914550781\n",
      "Diffusion Epoch 4 average loss: 2.6140695095062254\n",
      "Diffusion Epoch 5 average loss: 2.6161951065063476\n",
      "Diffusion Epoch 6 average loss: 2.6146123886108397\n",
      "Diffusion Epoch 7 average loss: 2.612207221984863\n",
      "Diffusion Epoch 8 average loss: 2.61614351272583\n",
      "Diffusion Epoch 9 average loss: 2.6164079189300535\n",
      "Diffusion Epoch 10 average loss: 2.6171091079711912\n",
      "Diffusion Epoch 11 average loss: 2.6193079471588137\n",
      "Diffusion Epoch 12 average loss: 2.619171380996704\n",
      "Diffusion Epoch 13 average loss: 2.616895151138306\n",
      "Diffusion Epoch 14 average loss: 2.6142040252685548\n",
      "Diffusion Epoch 15 average loss: 2.6132326126098633\n",
      "Diffusion Epoch 16 average loss: 2.6188111782073973\n",
      "Diffusion Epoch 17 average loss: 2.617199182510376\n",
      "Diffusion Epoch 18 average loss: 2.6089354515075684\n",
      "Diffusion Epoch 19 average loss: 2.6169989109039307\n",
      "Diffusion Epoch 20 average loss: 2.614582633972168\n",
      "Diffusion Epoch 21 average loss: 2.6139416217803957\n",
      "Diffusion Epoch 22 average loss: 2.6172975063323975\n",
      "Diffusion Epoch 23 average loss: 2.6119669914245605\n",
      "Diffusion Epoch 24 average loss: 2.619831943511963\n",
      "Diffusion Epoch 25 average loss: 2.6134763240814207\n",
      "Diffusion Epoch 26 average loss: 2.612503480911255\n",
      "Diffusion Epoch 27 average loss: 2.6167487621307375\n",
      "Diffusion Epoch 28 average loss: 2.6153863430023194\n",
      "Diffusion Epoch 29 average loss: 2.6072389125823974\n",
      "Diffusion Epoch 30 average loss: 2.6164536476135254\n",
      "Diffusion Epoch 31 average loss: 2.617221450805664\n",
      "Diffusion Epoch 32 average loss: 2.615559148788452\n",
      "Diffusion Epoch 33 average loss: 2.6159889698028564\n",
      "Diffusion Epoch 34 average loss: 2.6179633140563965\n",
      "Diffusion Epoch 35 average loss: 2.608991527557373\n",
      "Diffusion Epoch 36 average loss: 2.622824478149414\n",
      "Diffusion Epoch 37 average loss: 2.611388158798218\n",
      "Diffusion Epoch 38 average loss: 2.611041450500488\n",
      "Diffusion Epoch 39 average loss: 2.6140419006347657\n",
      "Diffusion Epoch 40 average loss: 2.6090327739715575\n",
      "Diffusion Epoch 41 average loss: 2.618599462509155\n",
      "Diffusion Epoch 42 average loss: 2.6185000419616697\n",
      "Diffusion Epoch 43 average loss: 2.6192402839660645\n",
      "Diffusion Epoch 44 average loss: 2.6199398040771484\n",
      "Diffusion Epoch 45 average loss: 2.622069835662842\n",
      "Diffusion Epoch 46 average loss: 2.6143610954284666\n",
      "Diffusion Epoch 47 average loss: 2.613881731033325\n",
      "Diffusion Epoch 48 average loss: 2.6177939891815187\n",
      "Diffusion Epoch 49 average loss: 2.616657257080078\n",
      "Diffusion Epoch 50 average loss: 2.6134949207305906\n",
      "Diffusion Epoch 51 average loss: 2.6039509773254395\n",
      "Diffusion Epoch 52 average loss: 2.6170074462890627\n",
      "Diffusion Epoch 53 average loss: 2.614304208755493\n",
      "Diffusion Epoch 54 average loss: 2.6091416835784913\n",
      "Diffusion Epoch 55 average loss: 2.616320848464966\n",
      "Diffusion Epoch 56 average loss: 2.6178553104400635\n",
      "Diffusion Epoch 57 average loss: 2.619074249267578\n",
      "Diffusion Epoch 58 average loss: 2.6149834632873534\n",
      "Diffusion Epoch 59 average loss: 2.6213570117950438\n",
      "Diffusion Epoch 60 average loss: 2.616768503189087\n",
      "Diffusion Epoch 61 average loss: 2.6198384761810303\n",
      "Diffusion Epoch 62 average loss: 2.620378017425537\n",
      "Diffusion Epoch 63 average loss: 2.618211555480957\n",
      "Diffusion Epoch 64 average loss: 2.6126762866973876\n",
      "Diffusion Epoch 65 average loss: 2.6160791873931886\n",
      "Diffusion Epoch 66 average loss: 2.61291618347168\n",
      "Diffusion Epoch 67 average loss: 2.611259651184082\n",
      "Diffusion Epoch 68 average loss: 2.610989713668823\n",
      "Diffusion Epoch 69 average loss: 2.6145817756652834\n",
      "Diffusion Epoch 70 average loss: 2.6131975173950197\n",
      "Diffusion Epoch 71 average loss: 2.620756912231445\n",
      "Diffusion Epoch 72 average loss: 2.612860822677612\n",
      "Diffusion Epoch 73 average loss: 2.6094457149505614\n",
      "Diffusion Epoch 74 average loss: 2.6168885231018066\n",
      "Diffusion Epoch 75 average loss: 2.612790107727051\n",
      "Diffusion Epoch 76 average loss: 2.6147838592529298\n",
      "Diffusion Epoch 77 average loss: 2.614752769470215\n",
      "Diffusion Epoch 78 average loss: 2.6149300575256347\n",
      "Diffusion Epoch 79 average loss: 2.6138445377349853\n",
      "Diffusion Epoch 80 average loss: 2.6069548606872557\n",
      "Diffusion Epoch 81 average loss: 2.6129262924194334\n",
      "Diffusion Epoch 82 average loss: 2.6134902477264403\n",
      "Diffusion Epoch 83 average loss: 2.611225128173828\n",
      "Diffusion Epoch 84 average loss: 2.6179480075836183\n",
      "Diffusion Epoch 85 average loss: 2.612434959411621\n",
      "Diffusion Epoch 86 average loss: 2.620151948928833\n",
      "Diffusion Epoch 87 average loss: 2.611902666091919\n",
      "Diffusion Epoch 88 average loss: 2.6168041229248047\n",
      "Diffusion Epoch 89 average loss: 2.612786626815796\n",
      "Diffusion Epoch 90 average loss: 2.6126044750213624\n",
      "Diffusion Epoch 91 average loss: 2.6157726764678957\n",
      "Diffusion Epoch 92 average loss: 2.6123534202575684\n",
      "Diffusion Epoch 93 average loss: 2.6133941650390624\n",
      "Diffusion Epoch 94 average loss: 2.6224253177642822\n",
      "Diffusion Epoch 95 average loss: 2.611734914779663\n",
      "Diffusion Epoch 96 average loss: 2.6159207820892334\n",
      "Diffusion Epoch 97 average loss: 2.616397762298584\n",
      "Diffusion Epoch 98 average loss: 2.6146653652191163\n",
      "Diffusion Epoch 99 average loss: 2.6167273998260496\n",
      "Debug(train_diffuser) :-> Diffusion Training Ended\n",
      "Debug(train_clone) :-> Generating Images using Diffusion Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It looks like you are trying to rescale already rescaled images. If the input images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug(train_clone) :-> Images Generated Using Diffusion Model\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "torch.Size([16, 3, 224, 224])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/300 [08:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m      3\u001b[0m        train_diffuser(args, diffuser_model, clone_model, scheduler_D, clone_processor)\n\u001b[1;32m----> 4\u001b[0m        \u001b[43mtrain_clone\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiffuser_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvictim_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_D\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_C\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvictim_processor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclone_processor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m        scheduler_lr\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[1;32mIn[14], line 28\u001b[0m, in \u001b[0;36mtrain_clone\u001b[1;34m(args, diffuser_model, clone_model, victim_model, scheduler_D, optimizer_C, victim_processor, clone_processor)\u001b[0m\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mcross_entropy(logits_T, logits_C)\n\u001b[0;32m     27\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m ) \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mgrad_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     30\u001b[0m     optimizer_C\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch22\\lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\torch22\\lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    " for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "     \n",
    "        train_diffuser(args, diffuser_model, clone_model, scheduler_D, clone_processor)\n",
    "        train_clone(args, diffuser_model, clone_model, victim_model, scheduler_D, optimizer_C, victim_processor, clone_processor)\n",
    "        scheduler_lr.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7b563f-926f-40c9-a64b-915ab21fb719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch22",
   "language": "python",
   "name": "torch22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "209a4dd9-14d1-450b-8762-db36db43ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "from kornia import augmentation\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from diffusers import UNet2DModel\n",
    "from datasets import load_dataset\n",
    "from torchvision import transforms\n",
    "from diffusers import DDPMScheduler\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from datasets import load_dataset, load_metric\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a83a42-7fa1-4069-a5ab-de1a26b60fbe",
   "metadata": {},
   "source": [
    "### Define Generator, Victim and Stolen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "058833bf-736f-48ab-b1b4-20cbb202d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        # Downsampling\n",
    "        for feature in features:\n",
    "            self.downs.append(DoubleConv(in_channels, feature))\n",
    "            in_channels = feature\n",
    "\n",
    "        # Upsampling\n",
    "        for feature in reversed(features):\n",
    "            self.ups.append(\n",
    "                nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2)\n",
    "            )\n",
    "            self.ups.append(DoubleConv(feature * 2, feature))\n",
    "\n",
    "        self.bottleneck = DoubleConv(features[-1], features[-1] * 2)\n",
    "        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip_connections = []\n",
    "\n",
    "        # Downsampling\n",
    "        for down in self.downs:\n",
    "            x = down(x)\n",
    "            skip_connections.append(x)\n",
    "            x = self.pool(x)\n",
    "\n",
    "        x = self.bottleneck(x)\n",
    "        skip_connections = skip_connections[::-1]\n",
    "\n",
    "        # Upsampling\n",
    "        for idx in range(0, len(self.ups), 2):\n",
    "            x = self.ups[idx](x)\n",
    "            skip_connection = skip_connections[idx // 2]\n",
    "\n",
    "            if x.shape != skip_connection.shape:\n",
    "                x = nn.functional.resize(x, size=skip_connection.shape[2:])\n",
    "\n",
    "            concat_skip = torch.cat((skip_connection, x), dim=1)\n",
    "            x = self.ups[idx + 1](concat_skip)\n",
    "\n",
    "        return self.final_conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c002cc9-5599-440c-81a0-abe667f37c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator(args):\n",
    "    class MultiStepGenerator(nn.Module):\n",
    "        def __init__(self, args):\n",
    "            super(MultiStepGenerator, self).__init__()\n",
    "            self.unet =UNet(in_channels=args.img_c, out_channels=args.img_c)\n",
    "            self.num_timesteps = args.num_timesteps\n",
    "        \n",
    "        def forward(self, noise):\n",
    "            images = []\n",
    "            current_image = noise\n",
    "    \n",
    "            for _ in range(self.num_timesteps):\n",
    "                current_image = self.unet(current_image)\n",
    "                images.append(current_image)\n",
    "            \n",
    "            return images\n",
    "\n",
    "    return MultiStepGenerator(args).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58c1448a-87e0-4758-b630-61a68a8e2a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Net, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.fc1 = nn.Linear(12544, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5bd5968-5922-4035-bfa2-75a3572b11c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetS(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(NetS, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "            self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "            self.fc1 = nn.Linear(12544, 128)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "            self.bn1 = nn.BatchNorm2d(32)\n",
    "            self.bn2 = nn.BatchNorm2d(64)\n",
    "    \n",
    "        def forward(self, x):\n",
    "            x = F.relu(self.conv1(x))\n",
    "            x = self.bn1(x)\n",
    "            x = F.relu(self.conv2(x))\n",
    "            x = self.bn2(x)\n",
    "            x = F.max_pool2d(x, 2)\n",
    "            x = torch.flatten(x, 1)\n",
    "            x = F.relu(self.fc1(x))\n",
    "            x = self.fc2(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5badb174-6c13-4dcb-b728-bffee591fec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_victim_clone(args):\n",
    "    \n",
    "    student = NetS().to(args.device)\n",
    "    victim = torch.load(args.victim_path, map_location=args.device)\n",
    "    return victim , student"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f7b72a-4a58-4a63-9dd1-db77d69bafec",
   "metadata": {},
   "source": [
    "### Utilites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a72d70bf-5c0f-4601-a990-44ceaa0ae003",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self, device, seed, epochs, batch_size,num_timesteps, img_n, img_c, momentum,\n",
    "                 img_w, img_h, lr_G, lr_C, lr_hee, lr_z, weight_decay, N_G, N_C, \n",
    "                 steps_hee, grad_accumulation_steps, std_aug, lam, label_smooth_factor,\n",
    "                 victim_path, N_classes, debug, save_dir):\n",
    "        \n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.num_timesteps = num_timesteps\n",
    "        self.img_n = img_n\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.lr_G = lr_G\n",
    "        self.lr_C = lr_C\n",
    "        self.lr_hee = lr_hee\n",
    "        self.lr_z = lr_z\n",
    "        self.weight_decay = weight_decay\n",
    "        self.momentum = momentum\n",
    "        self.N_G = N_G\n",
    "        self.N_C = N_C\n",
    "        self.steps_hee = steps_hee\n",
    "        self.grad_accumulation_steps = grad_accumulation_steps\n",
    "        self.std_aug = std_aug\n",
    "        self.lam = lam\n",
    "        self.victim_path = victim_path\n",
    "        self.N_classes = N_classes\n",
    "        self.debug = debug\n",
    "        self.victim_path = victim_path\n",
    "        self.label_smooth_factor = label_smooth_factor\n",
    "        self.save_dir = save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3acc036a-a271-4620-a5ba-22c104ac0b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Some Information about FakeDataset\"\"\"\n",
    "\n",
    "    def __init__(self, root=\"\", transform=None):\n",
    "        super(FakeDataset, self).__init__()\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        history_images = np.load(os.path.join(root, \"fake_images.npy\"))\n",
    "        self.images = torch.from_numpy(history_images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = self.images[index]\n",
    "\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5dbcf080-786f-4378-8e0f-719bc6240b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorDataset(Dataset):\n",
    "    def __init__(self, tensor):\n",
    "        self.tensor = tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tensor)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.tensor[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90760983-d005-439a-b334-28565629571a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIter(object):\n",
    "    def __init__(self, dataloader):\n",
    "        self.dataloader = dataloader\n",
    "        self._iter = iter(self.dataloader)\n",
    "\n",
    "    def next(self):\n",
    "        try:\n",
    "            data = next(self._iter)\n",
    "        except StopIteration:\n",
    "            self._iter = iter(self.dataloader)\n",
    "            data = next(self._iter)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "283f433e-1666-439c-9120-40ab921109ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_standard_augment(img_w, img_h):\n",
    "    std_aug = augmentation.container.ImageSequential(\n",
    "    augmentation.RandomCrop(size=[img_w, img_h], padding=4),\n",
    "    augmentation.RandomHorizontalFlip(),\n",
    ")\n",
    "    return std_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bc23b46-cae7-4f8a-a471-986e7a97d28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strong_aug(image):\n",
    "    device = image.device\n",
    "    image = TF.center_crop(\n",
    "        image,\n",
    "        [int(32.0 * random.uniform(0.95, 1.0)), int(32.0 * random.uniform(0.95, 1.0))],\n",
    "    )\n",
    "    image = TF.resize(image, [32, 32])\n",
    "    noise = torch.randn_like(image).to(device) * 0.001\n",
    "    image = torch.clamp(image + noise, 0.0, 1.0)\n",
    "    if random.uniform(0, 1) > 0.5:\n",
    "        image = TF.vflip(image)\n",
    "    if random.uniform(0, 1) > 0.5:\n",
    "        image = TF.hflip(image)\n",
    "    angles = [-15, 0, 15]\n",
    "    angle = random.choice(angles)\n",
    "    image = TF.rotate(image, angle)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41ea93c1-1fec-4a94-bba0-fe084544b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_one_hot(args, true_labels):\n",
    "    \"\"\"\n",
    "    if smoothing == 0, it's one-hot method\n",
    "    if 0 < smoothing < 1, it's smooth method\n",
    "    \"\"\"\n",
    "    device = true_labels.device\n",
    "    true_labels = torch.nn.functional.one_hot(true_labels, args.N_classes).detach().cpu()\n",
    "    assert 0 <= args.label_smooth_factor < 1\n",
    "    confidence = 1.0 - args.label_smooth_factor\n",
    "    label_shape = torch.Size((true_labels.size(0), args.N_classes))\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.empty(size=label_shape, device=true_labels.device)\n",
    "        true_dist.fill_(args.label_smooth_factor / (args.N_classes - 1))\n",
    "        _, index = torch.max(true_labels, 1)\n",
    "\n",
    "        true_dist.scatter_(1, torch.LongTensor(index.unsqueeze(1)), confidence)\n",
    "    return true_dist.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c27ac394-f91a-4698-8a3a-ad5f3c5c0819",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(net, testloader):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = net(images.to(device))\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.to(device)).sum().item()\n",
    "    \n",
    "    print(f'Accuracy of the network on the 10,000 test images: {100 * correct / total:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "eb7ce857-db53-4dfb-88ae-ea5e4324d282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_batch_fake(args, images, epoch):\n",
    "    images = images.detach().cpu().numpy()\n",
    "    images_filename = os.path.join(args.save_dir, \"fake_images.npy\")\n",
    "\n",
    "    if epoch > 1:\n",
    "        org_images = np.load(images_filename)\n",
    "\n",
    "        images = np.concatenate((org_images, images), 0)\n",
    "        \n",
    "    np.save(images_filename, images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "75df346e-60d5-4525-bcd9-90e7065934d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the Gram matrix\n",
    "def gram_matrix(x, mask = None):\n",
    "    A = torch.matmul(x, x.t())\n",
    "    if mask == None:\n",
    "        return A\n",
    "    else:\n",
    "        return torch.tril(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d75aaf-ae69-479d-9483-32cacc82991c",
   "metadata": {},
   "source": [
    "### DFME++ Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb06396f-c99b-47c5-b427-16cd013b2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: L1 loss between student and teacher Gram matrices\n",
    "def l1_loss_between_grams(student_probs, teacher_probs, mask = None):  # Student prob : time steps, classes. \n",
    "    student_gram = gram_matrix(student_probs, mask)\n",
    "    teacher_gram = gram_matrix(teacher_probs, mask)\n",
    "    return F.l1_loss(student_gram, teacher_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2614f5c-9a4b-4441-b4f8-028b44634bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entropy_Loss(nn.Module):\n",
    "    def __init__(self, reduction=\"mean\"):\n",
    "        super(Entropy_Loss, self).__init__()\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, x):  # x should be logits\n",
    "        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "        b = 1.0 * b.sum(dim=1)\n",
    "        if self.reduction == \"mean\":\n",
    "            return b.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return b.sum()\n",
    "        elif self.reduction == \"none\":\n",
    "            return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b23c3df-aa05-4889-b862-b7fe825c6074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal Loss: 0.08231411874294281\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
    "        \"\"\"\n",
    "        Focal Loss for multi-class classification.\n",
    "        \n",
    "        :param alpha: (float) balancing factor for class imbalance (default: 1)\n",
    "        :param gamma: (float) focusing parameter (default: 2)\n",
    "        :param reduction: (str) reduction method to apply to the output: 'none' | 'mean' | 'sum'\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Forward pass for Focal Loss.\n",
    "        \n",
    "        :param inputs: (Tensor) Predictions from the model (logits), shape (batch_size, num_classes)\n",
    "        :param targets: (Tensor) Ground truth labels, shape (batch_size)\n",
    "        :return: (Tensor) Computed Focal Loss\n",
    "        \"\"\"\n",
    "        # Convert targets to one-hot encoding\n",
    "        targets = F.one_hot(targets, num_classes=inputs.size(1)).float()\n",
    "\n",
    "        # Apply softmax to get probabilities\n",
    "        probabilities = F.softmax(inputs, dim=1)\n",
    "\n",
    "        # Compute the log of probabilities\n",
    "        log_p = torch.log(probabilities)\n",
    "\n",
    "        # Compute the focal loss component\n",
    "        loss = -targets * (1 - probabilities) ** self.gamma * log_p\n",
    "\n",
    "        # Apply alpha (balancing factor)\n",
    "        loss = self.alpha * loss\n",
    "\n",
    "        # Reduce the loss\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Simulated logits and target labels\n",
    "    logits = torch.tensor([[0.2, 0.3, 0.5], [0.1, 0.8, 0.1], [0.7, 0.2, 0.1]], dtype=torch.float32)\n",
    "    targets = torch.tensor([2, 1, 0], dtype=torch.int64)  # Ground truth labels\n",
    "\n",
    "    # Instantiate the focal loss\n",
    "    focal_loss = FocalLoss(alpha=1, gamma=2, reduction='mean')\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = focal_loss(logits, targets)\n",
    "    print(f\"Focal Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6fb03a-2634-4e34-8a89-2229f5096131",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8b673b2-1487-401f-8c21-215c8810dc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logodds(logits):\n",
    "    eps = 1e-7\n",
    "    softmax_odds = F.softmax(logits, dim=1)\n",
    "    softmax_odds = torch.clamp(softmax_odds, min = eps, max = 1-eps)\n",
    "    log_odd = torch.log((softmax_odds) / (1-softmax_odds) )\n",
    "    log_sigmoid_odd = torch.nn.LogSigmoid()\n",
    "    return -log_sigmoid_odd(log_odd).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5abf25b-307f-4a83-bc46-fc8ce8b3136b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def cross_entropy(outputs, smooth_labels):\n",
    "#     loss = torch.nn.KLDivLoss(reduction=\"batchmean\")\n",
    "#     return loss(F.log_softmax(outputs, dim=1), smooth_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "34ae25b1-adc2-4c90-b107-e5fe5d2dea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_hee(args, model, x):\n",
    "#     model.eval()\n",
    "#     x_hee = x.detach() + 0.001 * torch.torch.randn(x.shape).to(args.device).detach()\n",
    "#     for _ in range(args.steps_hee):\n",
    "#         x_hee.requires_grad_()\n",
    "#         with torch.enable_grad():\n",
    "#             pred = model(x_hee)\n",
    "#             loss = Entropy_Loss(reduction=\"mean\")(pred)\n",
    "#         grad = torch.autograd.grad(loss, [x_hee])[0] \n",
    "#         x_hee = x_hee.detach() + args.lr_hee * torch.sign(grad.detach())\n",
    "#         x_hee = torch.clamp(x_hee, 0.0, 1.0)\n",
    "#     model.train()\n",
    "\n",
    "#     return x_hee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "24d8f970-f4a2-41c0-a3d5-0394941a31a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator(args, generator_model, clone_model, epoch):\n",
    "\n",
    "    generator_model.train()\n",
    "    clone_model.eval()\n",
    "    \n",
    "    best_fake = None\n",
    "    best_loss = 1e6\n",
    "    \n",
    "    if args.debug: print('Debug(train_generator) :-> Generating Images using Generator Model')\n",
    "    # z.requires_grad = True\n",
    "    \n",
    "    optimizer_G = torch.optim.Adam([{\"params\": generator_model.parameters()}], lr=args.lr_G, betas=[0.5, 0.999])\n",
    "    # pseudo_y = torch.randint(low=0, high=args.N_classes, size=(args.img_n,)).to(args.device)\n",
    "    # soft_labels = smooth_one_hot(args, pseudo_y)\n",
    "    criterion = Entropy_Loss()\n",
    "\n",
    "    loss_avg = []\n",
    "    \n",
    "    if args.debug: print('Debug(train_generator) :-> Starting Generator Training')\n",
    "    for step in range(args.N_G):\n",
    "        z = torch.randn(size=(args.img_n, args.img_c,args.img_w, args.img_h )).to(args.device)\n",
    "        fake = torch.stack(generator_model(z)).squeeze(1)\n",
    "        # aug_fake = args.std_aug(fake)  # data augmentation to fake images. \n",
    "        \n",
    "        logits = clone_model(fake)\n",
    "        # print(logits.shape)\n",
    "        loss = criterion(logits)\n",
    "\n",
    "        loss_avg.append(loss.item())\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            if best_loss > loss.item() or best_fake is None:\n",
    "                best_loss = loss.item()\n",
    "                best_fake = fake\n",
    "        \n",
    "        optimizer_G.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm(clone_model.parameters(), max_norm = 1)\n",
    "        optimizer_G.step()\n",
    "\n",
    "    \n",
    "    print(f\"Generator Step {step} average loss: {np.mean(loss_avg)}\")\n",
    "    # print(best_fake)\n",
    "    save_batch_fake(args, best_fake.data, epoch)\n",
    "    if args.debug: print('Debug(train_generator) :-> Generator Training Ended')\n",
    "    return generator_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "39ca31e0-8315-459e-bc2b-f013c5091674",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_clone(args, generator_model, clone_model, victim_model, optimizer_C):\n",
    "\n",
    "    generator_model.eval()\n",
    "    clone_model.train()\n",
    "    victim_model.eval()\n",
    "    \n",
    "    dataset = FakeDataset(root=args.save_dir)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=args.batch_size,\n",
    "        shuffle=True,\n",
    "    )\n",
    "    data_iter = DataIter(data_loader)\n",
    "\n",
    "    print(f\"data loader {len(data_loader)}\")\n",
    "    \n",
    "    victim_classes_fired = np.zeros(args.N_classes)\n",
    "    clone_classes_fired = np.zeros(args.N_classes)\n",
    "    losses = []\n",
    "    if args.debug: print('Debug(train_clone) :-> Starting Clone Model Training')    \n",
    "    for step in range(args.N_C):\n",
    "        loss_avg = 0\n",
    "        for data in range(len(data_loader)):\n",
    "        \n",
    "            fake = data_iter.next().to(args.device)\n",
    "            # aug_fake = args.std_aug(fake)  # already applied before saving\n",
    "            \n",
    "            # fake_hee = generate_hee(args, clone_model,strong_aug(aug_fake))\n",
    "            \n",
    "            logits_T = victim_model(fake).detach() \n",
    "            # print(logits.T)\n",
    "            hard_labels = logits_T.topk(1, 1)[1].reshape(-1)\n",
    "            victim_probs = F.softmax(logits_T)\n",
    "            np.add.at(victim_classes_fired, hard_labels.cpu().numpy() , 1)\n",
    "            \n",
    "            logits_C = clone_model(fake)\n",
    "            clone_probs = F.softmax(logits_C)\n",
    "            np.add.at(clone_classes_fired, logits_C.topk(1, 1)[1].reshape(-1).cpu().numpy() , 1)\n",
    "    \n",
    "            mask = None # \"triangular\" \n",
    "            loss = l1_loss_between_grams(clone_probs, victim_probs, mask)\n",
    "            loss_avg+=loss\n",
    "\n",
    "        loss_avg = loss_avg/len(data_loader)\n",
    "            \n",
    "        losses.append(loss_avg.item())\n",
    "        optimizer_C.zero_grad()\n",
    "        loss_avg.backward()\n",
    "        optimizer_C.step()\n",
    "\n",
    "    \n",
    "    # print(f\"Clone Steps {step} average loss: {sum(losses[-len(data_loader):])/len(data_loader)}\")\n",
    "    print(f\"Clone Steps {step} average loss: {np.mean(losses)}\")\n",
    "\n",
    "    print(f\"Clone Steps {step} Victim Classes Fired: {victim_classes_fired})\")\n",
    "    print(f\"Clone Steps {step} Clone Classes Fired: {clone_classes_fired})\")\n",
    "    if args.debug: print('Debug(train_clone) :-> Clone Model Training Ended') \n",
    "    return (clone_model, victim_model, optimizer_C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "9fb4a0d0-24b1-4475-b383-2091324cdc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = 1 #To debug code\n",
    "device = torch.device('cuda') #device placement cpu or gpu\n",
    "seed = 10 #seed for consistent result\n",
    "epochs = 10 #number of epochs to train\n",
    "batch_size = 64 #batchsize\n",
    "num_timesteps = 100 #timestamps in multistepgenerator\n",
    "img_n = 1\n",
    "img_c = 1 #image channel\n",
    "img_w = 32 #image size\n",
    "img_h = 32 #image size\n",
    "lr_z = 0.01 #learning rate of latent code\n",
    "lr_G = 1e-6 #learning rate of Generator\n",
    "lr_C = 1e-6 #learing rate of clone model\n",
    "lr_hee = 0.03 #perturb number of steps\n",
    "weight_decay = 1e-4 #Optimizer parameter: decay's weight update\n",
    "momentum = 0.9 #Optimizer parameter: Remeber past information 1/momentum times\n",
    "N_G = 5 #Diffuser train steps\n",
    "N_C = 1 #Clone model steps \n",
    "steps_hee = 10 #number of epochs to train\n",
    "grad_accumulation_steps = 16 #update model after no.of steps\n",
    "std_aug = get_standard_augment(img_w, img_h) #standard augmentation: flip, crop\n",
    "lam = 0.009 #hyperparameter for balancing two loss terms in diffuser\n",
    "victim_path = \"Model/fashion_mnist.pt\" #clone model path\n",
    "N_classes = 10 #No.of classes to predict\n",
    "label_smooth_factor = 0.2 #Hard to soft logits\n",
    "save_dir = r'Gen_Imgs/Gram' #Folder to save genrator images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "bf41408d-6de7-4bbe-b9fb-8c3e2e641a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Args(\n",
    "        debug = debug,\n",
    "        device = device,\n",
    "        seed = seed,\n",
    "        epochs = epochs,\n",
    "        batch_size = batch_size,\n",
    "        num_timesteps = num_timesteps,\n",
    "        img_n = img_n,\n",
    "        img_c = img_c,\n",
    "        img_w = img_w,\n",
    "        img_h = img_h,\n",
    "        lr_G = lr_G,\n",
    "        lr_C = lr_C,\n",
    "        lr_z = lr_z,\n",
    "        lr_hee = lr_hee,\n",
    "        weight_decay = weight_decay,\n",
    "        momentum = momentum,\n",
    "        N_G = N_G,\n",
    "        N_C = N_C,\n",
    "        steps_hee = steps_hee,\n",
    "        grad_accumulation_steps = grad_accumulation_steps,\n",
    "        std_aug = std_aug,\n",
    "        lam = lam,\n",
    "        victim_path = victim_path,\n",
    "        N_classes = N_classes,\n",
    "        label_smooth_factor = label_smooth_factor,\n",
    "        save_dir = save_dir\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "84d79aed-b9c5-4f0a-ab98-646d8c2a7fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generator_model =  get_generator(args)\n",
    "victim_model, clone_model = get_victim_clone(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1d587785-14be-4afe-a16a-f6b15b88d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "18130477-043a-4282-ace6-554523132498",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_C = torch.optim.AdamW(\n",
    "        clone_model.parameters(),\n",
    "        lr=args.lr_C,\n",
    "        weight_decay=args.weight_decay,\n",
    "    )\n",
    "scheduler_lr = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer_C, args.epochs, eta_min=2e-4\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "2ac01e74-082d-4bb8-96aa-653e01dee40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Resize((32, 32))])\n",
    "testset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = DataLoader(testset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8a070d09-a52b-4139-9784-583a861ffc35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset FashionMNIST\n",
       "    Number of datapoints: 10000\n",
       "    Root location: ./data\n",
       "    Split: Test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               ToTensor()\n",
       "               Resize(size=(32, 32), interpolation=bilinear, max_size=None, antialias=True)\n",
       "           )"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5f35c1fa-02e1-49c6-90cf-3527b91e0510",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 92.07%\n",
      "Accuracy of the network on the 10,000 test images: 10.39%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(victim_model, testloader), evaluate(clone_model, testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd7e96c-9df4-4f02-a4db-21fcfca02c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                    | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -2.3011075496673583\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 2\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.6639330387115479\n",
      "Clone Steps 0 Victim Classes Fired: [ 1.  0.  0.  4.  0. 10.  0.  0. 85.  0.])\n",
      "Clone Steps 0 Clone Classes Fired: [32.  0.  5. 13. 12.  1. 12.  3. 16.  6.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█████████████████▏                                                                                                                                                          | 1/10 [00:10<01:34, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -0.5703752160072326\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 4\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.5859357118606567\n",
      "Clone Steps 0 Victim Classes Fired: [  2.   0.   2.   7.   0.  26.   0.   0. 163.   0.])\n",
      "Clone Steps 0 Clone Classes Fired: [74.  0. 16. 27. 24.  2. 16.  5. 26. 10.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██████████████████████████████████▍                                                                                                                                         | 2/10 [00:21<01:24, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -0.5878498077392578\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 5\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.6202067732810974\n",
      "Clone Steps 0 Victim Classes Fired: [  4.   1.   3.   8.   0.  41.   0.   0. 243.   0.])\n",
      "Clone Steps 0 Clone Classes Fired: [115.   0.  24.  37.  28.   3.  26.   9.  46.  12.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████████████████████████████████▌                                                                                                                        | 3/10 [00:31<01:13, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -0.591933524608612\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 7\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.6344326734542847\n",
      "Clone Steps 0 Victim Classes Fired: [  5.   1.   4.   8.   0.  52.   0.   0. 330.   0.])\n",
      "Clone Steps 0 Clone Classes Fired: [163.   0.  28.  53.  44.   6.  29.   9.  56.  12.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████████████████████████████████████████████████████████████████████▊                                                                                                       | 4/10 [00:42<01:03, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -0.5688566923141479\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 8\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.6276766061782837\n",
      "Clone Steps 0 Victim Classes Fired: [  7.   2.   8.  11.   0.  61.   1.   0. 410.   0.])\n",
      "Clone Steps 0 Clone Classes Fired: [214.   0.  42.  67.  53.   8.  31.  11.  62.  12.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|██████████████████████████████████████████████████████████████████████████████████████                                                                                      | 5/10 [00:52<00:52, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -0.59895099401474\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 10\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.6248451471328735\n",
      "Clone Steps 0 Victim Classes Fired: [  9.   2.   8.  13.   0.  79.   2.   0. 487.   0.])\n",
      "Clone Steps 0 Clone Classes Fired: [257.   0.  51.  78.  59.   9.  43.  14.  73.  16.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                    | 6/10 [01:03<00:42, 10.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -0.614935839176178\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 11\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.6353253722190857\n",
      "Clone Steps 0 Victim Classes Fired: [  9.   2.   9.  13.   1.  84.   4.   0. 578.   0.])\n",
      "Clone Steps 0 Clone Classes Fired: [306.   0.  62.  90.  76.  10.  42.  15.  83.  16.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                   | 7/10 [01:14<00:32, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n",
      "Generator Step 4 average loss: -0.5859798312187194\n",
      "Debug(train_generator) :-> Generator Training Ended\n",
      "--------------------------------------------------\n",
      "data loader 13\n",
      "Debug(train_clone) :-> Starting Clone Model Training\n",
      "Clone Steps 0 average loss: 0.6171724200248718\n",
      "Clone Steps 0 Victim Classes Fired: [  9.   5.   9.  17.   1. 107.   5.   0. 647.   0.])\n",
      "Clone Steps 0 Clone Classes Fired: [351.   0.  71. 109.  80.  12.  48.  15.  98.  16.])\n",
      "Debug(train_clone) :-> Clone Model Training Ended\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                  | 8/10 [01:24<00:21, 10.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10,000 test images: 12.21%\n",
      "\n",
      "Debug(train_generator) :-> Generating Images using Generator Model\n",
      "Debug(train_generator) :-> Starting Generator Training\n"
     ]
    }
   ],
   "source": [
    " for epoch in tqdm(range(1, args.epochs + 1)):\n",
    "     \n",
    "    generator_model = train_generator(args, generator_model, clone_model, epoch)\n",
    "    print('-'*50)\n",
    "    clone_model, victim_model, optimizer_C = train_clone(args, generator_model, clone_model, victim_model, optimizer_C)\n",
    "    print('-'*50)\n",
    "    # print(scheduler_lr.get_last_lr())\n",
    "    print()\n",
    "    evaluate(clone_model, testloader)\n",
    "    print()\n",
    "    scheduler_lr.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8bb994-3d88-48ad-bdba-5f0ff252be97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DFME)",
   "language": "python",
   "name": "dfme"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
